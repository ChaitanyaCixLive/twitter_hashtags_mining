{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags databese\n",
    "Take each single hashtag and compute the lexical diversity of the text it appeard in.\n",
    "\n",
    "We want to create a parallel database with information relate to the single hashtags.\n",
    "This will contain:\n",
    "- Lexical diversity of the corpus of tweets the hashtag compare in\n",
    "- Average number of words\n",
    "- Mean sentiment of the corpus of tweets the hashtag compare in\n",
    "- Sentiment variance of the corpus of tweets the hashtag compare in\n",
    "- Mean retweet count of the corpus of tweets the hashtag compare in\n",
    "- Retweet count variance of the corpus of tweets the hashtag compare in\n",
    "In this collection the id is represented by the unique hashtag, followed by all the above mentioned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First clean the set of hashtags collected by lowering them down (lower cases)\n",
    "We don't want '#ecig' and '#ECig' to be counted as two different hastags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of all the unique hashtags in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.sn_sp\n",
    "coll = db.net_1\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "items_list = [ item for item in cursor ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_hashstags = []\n",
    "\n",
    "for tweet_doc in items_list:\n",
    "    for h in tweet_doc['hashtags']:\n",
    "        unique_hashstags.append(h)\n",
    "\n",
    "unique_hashstags = list(set(unique_hashstags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56352\n",
      "[u'cafeaalto', u'eatnaturally', u'rakije', u'placedesarts', u'woods', u'tommysotomayar', u'vapesatgram', u'\\u65e9\\u826f\\u5e7c\\u7a1a\\u5712', u'cuckolding', u'katrinakaif', u'\\u897f\\u65e5\\u672c', u'\\u30d3\\u30e5\\u30fc\\u30c6\\u30a3', u'14kt', u'fu\\xdfg\\xe4ngertunnels', u'xandra', u'wheatbelly', u'\\u9ad8\\u53ce\\u5165', u'joanofarc', u'hermano', u'thomasville']\n"
     ]
    }
   ],
   "source": [
    "print len(unique_hashstags)\n",
    "print unique_hashstags[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each if the unique hashtags query the database and create a new collection where each hashtag compare as ID and is followed by: the list of texts it compares in, the list of sentiments of the texts it compares in, the list of retweet count of the tweets it compares in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.sn_sp\n",
    "coll1 = db.net_1\n",
    "coll2 = db.hashtags_1_count\n",
    "\n",
    "for h in unique_hashstags:\n",
    "    h_features = {'_id':h, 'sentiments':[], 'texts':[], 'retweet_counts':[]}\n",
    "    cursor = coll1.find({'hashtags':h})\n",
    "    for tweet_doc in cursor:\n",
    "        h_features['sentiments'].append(tweet_doc['sentiment'])\n",
    "        h_features['texts'].append(tweet_doc['text'])\n",
    "        h_features['retweet_counts'].append(tweet_doc['retweet_count'])\n",
    "    coll2.insert_one(h_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute: top 10 words, lexical diversity, averge number of words, mean/vaiance of the sentiment (plus count of the sentiments), mean/variance of retweet count and create the final hashtags collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLP cleaning functions\n",
    "\n",
    "def processTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert retweet\n",
    "    tweet = re.sub('(rt\\s)@[^\\s]+','RETWEET',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def processWord(w):\n",
    "    #look for 2 or more repetitions of character in a word and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    w = pattern.sub(r\"\\1\\1\", w)\n",
    "    #strip punctuation\n",
    "    w = w.strip('\\'\"?,.')\n",
    "    #check if the word starts with an alphabet\n",
    "    val = re.search(r\"^[a-zA-Z][a-zA-Z0-9-]*$\", w)\n",
    "    if val is None:\n",
    "        w = 'ABC'\n",
    "    return w\n",
    "\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    st = open(stopWordListFileName, 'r')\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    stopWords.append('RETWEET')\n",
    "    stopWords.append('ABC')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "\n",
    "def getWordsVector(tweet):\n",
    "    # initialize vector\n",
    "    wordsVector = []\n",
    "    #initialize stopWords\n",
    "    stopWords = getStopWordList('/Users/nicolavitale/Desktop/twitter_data_analysis/develop/data/SmartStoplist.txt')\n",
    "    #process tweet and split into words\n",
    "    tweet = processTweet(tweet)\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = processWord(w)\n",
    "        if w in stopWords:\n",
    "            continue\n",
    "        else:\n",
    "            wordsVector.append(w.lower())\n",
    "    return wordsVector\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "def top10_words(statuses):\n",
    "    words = []\n",
    "    for s in statuses:\n",
    "        wordsVector = getWordsVector(s)\n",
    "        for word in wordsVector:\n",
    "            words.append(word)\n",
    "    c = Counter(words)\n",
    "    return list(c.most_common()[:10])\n",
    "\n",
    "    \n",
    "\n",
    "def lexical_diversity(statuses):\n",
    "    words = []\n",
    "    for s in statuses:\n",
    "        wordsVector = s.split()\n",
    "        for word in wordsVector:\n",
    "            words.append(word.lower())\n",
    "    return 1.0*len(set(words))/len(words) \n",
    "\n",
    "def average_words(statuses):\n",
    "    statuses = [s.lower() for s in statuses]\n",
    "    total_words = sum([ len(s.split()) for s in statuses ])\n",
    "    return 1.0*total_words/len(statuses)\n",
    "\n",
    "def mean_sentiment(sentiments):\n",
    "    sentiments_array = np.asarray(sentiments)\n",
    "    return np.mean(sentiments_array)\n",
    "\n",
    "def sentiment_variance(sentiments):\n",
    "    sentiments_array = np.asarray(sentiments)\n",
    "    return np.var(sentiments_array)\n",
    "\n",
    "def count_positive(sentiments):\n",
    "    return sentiments.count(1)\n",
    "\n",
    "def count_negative(sentiments):\n",
    "    return sentiments.count(0)\n",
    "\n",
    "def mean_rtcount(retweet_counts):\n",
    "    retweet_counts_array = np.asarray(retweet_counts)\n",
    "    return np.mean(retweet_counts_array)\n",
    "\n",
    "def rtcount_variance(retweet_counts):\n",
    "    retweet_counts_array = np.asarray(retweet_counts)\n",
    "    return np.var(retweet_counts_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.sn_sp\n",
    "coll1 = db.hashtags_1_count\n",
    "coll2 = db.hashtags_2_count\n",
    "\n",
    "cursor = coll1.find({})\n",
    "\n",
    "for h in cursor:\n",
    "    \n",
    "    h_features = {'_id':h['_id'], 'top_10_words':[], 'lexical_diversity':0, 'average_words_n':0 , 'mean_sentiment':0, 'sentiment_variance':0, 'mean_rtcount':0, 'rtcount_variance':0}\n",
    "    \n",
    "    h_features['top_10_words']=top10_words(h['texts'])\n",
    "    h_features['lexical_diversity']=lexical_diversity(h['texts'])\n",
    "    h_features['average_words_n']=average_words(h['texts'])\n",
    "    h_features['mean_sentiment']=mean_sentiment(h['sentiments'])\n",
    "    h_features['sentiment_variance']=sentiment_variance(h['sentiments'])\n",
    "    h_features['positive_count']=count_positive(h['sentiments'])\n",
    "    h_features['negative_count']=count_negative(h['sentiments'])\n",
    "    h_features['mean_rtcount']=mean_rtcount(h['retweet_counts'])\n",
    "    h_features['rtcount_variance']=rtcount_variance(h['retweet_counts'])\n",
    "    \n",
    "    coll2.insert_one(h_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move tracked hashtags into final collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Desktop/twitter_data_analysis/develop/Py/tracked.csv') \n",
    "tracked_list = df['x'].tolist()\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "db = client.sn_sp\n",
    "coll1 = db.hashtags_2_count\n",
    "coll2 = db.hashtags_tracked_final_count\n",
    "\n",
    "final_list = []\n",
    "\n",
    "for h in tracked_list:\n",
    "    tracked_h = coll1.find({\"_id\" : h})\n",
    "    for i in tracked_h:\n",
    "        final_list.append(i)\n",
    "\n",
    "for i in final_list:\n",
    "    coll2.insert_one(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
