{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply proper algorithm for sentiment classification\n",
    "\n",
    "Possible algorithms:\n",
    "\n",
    "- Naive Bayes\n",
    "    * Web resources\n",
    "        * [how to build a twitter sentiment analyzer ?](http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/)\n",
    "        * [Twitter Sentimental Analysis](https://github.com/victorneo/Twitter-Sentimental-Analysis)\n",
    "    * Papers\n",
    "        * [Twitter Sentiment Classification using Distant Supervision](http://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)\n",
    "\n",
    "__Note__: \n",
    "\n",
    "-----------\n",
    "\n",
    "_ROUGH_: use \"Twitter Sentimental Analysis\" which implement Naive Bayes within NLTK.         \n",
    "_FINAL DECISION_: Improve Naive Bayes using tesi unibo OR try another classifier in \"how to build a twitter sentiment analyzer ?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess tweets\n",
    "1. Lower Case - Convert the tweets to lower case.\n",
    "2. URLs - I don't intend to follow the short urls and determine the content of the site, so we can eliminate all of these URLs via regular expression matching or replace with generic word URL.\n",
    "3. @username - we can eliminate \"@username\" via regex matching or replace it with generic word AT_USER.\n",
    "4. #hashtag - hash tags can give us some useful information, so it is useful to replace them with the exact same word without the hash. E.g. #nike replaced with 'nike'.\n",
    "5. Punctuations and additional white spaces - remove punctuation at the start and ending of the tweets. E.g: ' the day is beautiful! ' replaced with 'the day is beautiful'. It is also helpful to replace multiple whitespaces with a single whitespace\n",
    "6. EMOTICON: gather positive emoticon and sibstitue with \"posmaremo\" and negative with \"negmaremo\". # TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# posmaremo\n",
    "# '(: :) :] [: :-) (-: [-: :-] (; ;) ;] [; ;-) (-; [-; ;-] :-D :D :-p :p (=: ;=D :=) :S @-) XD' \n",
    "# negmaremo\n",
    "# '\\:\\(|\\)\\:|:\\-\\(|\\)\\-\\:|\\;\\(|\\)\\;|\\:\\-\\[|\\]\\-\\:|\\;\\-\\(|\\)\\-\\;|\\:\\'\\[|\\:\\'\\(|\\)\\'\\:|\\]\\:|\\:\\[|:\\||\\:\\/|\\|\\:|\\/\\:|\\:\\=\\(|\\:\\=\\||\\:\\=\\[|xo|D\\:|O\\:'\n",
    "def processTweet(tweet):\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert retweet\n",
    "    tweet = re.sub('(rt\\s)@[^\\s]+','RT',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    # Group emoticons\n",
    "    # detection emoticon positive :-), :D ...\n",
    "    tweet = re.sub('\\(\\:|\\:\\)|\\:\\]|\\[\\:|\\:\\-\\)|\\(\\-\\:|\\[\\-\\:|\\:\\-\\]|\\(\\;|\\;\\)|\\;\\]|\\[\\;|\\;\\-\\)|\\(\\-\\;|\\[\\-\\;|\\;\\-\\]|\\:\\-D|:D|\\:\\-p|\\:p|\\(\\=\\:|\\;\\=D|\\:\\=\\)|\\:S|@\\-\\)|XD','posmaremo',tweet)\n",
    "    # detection emoticon negative )-:, :-/ ...\n",
    "    tweet = re.sub('\\:\\(|\\)\\:|:\\-\\(|\\)\\-\\:|\\;\\(|\\)\\;|\\:\\-\\[|\\]\\-\\:|\\;\\-\\(|\\)\\-\\;|\\:\\'\\[|\\:\\'\\(|\\)\\'\\:|\\]\\:|\\:\\[|:\\||\\:\\/|\\|\\:|\\/\\:|\\:\\=\\(|\\:\\=\\||\\:\\=\\[|xo|D\\:|O\\:','negmaremo',tweet)\n",
    "    # Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering tweet words and obtaining features vector\n",
    "__Note__: it is not effitient to completely clean the tweet; it has to be processed as a whole string, then the single words are processed when extracted to create the features vector.\n",
    "1. Stop words - a, is, the, with etc. The full list of stop words can be found at Stop Word List. These words don't indicate any sentiment and can be removed.\n",
    "2. Repeating letters - if you look at the tweets, sometimes people repeat letters to stress the emotion. E.g. hunggrryyy, huuuuuuungry for 'hungry'. We can look for 2 or more repetitive letters in words and replace them by 2 of the same.\n",
    "3. Punctuation - we can remove punctuation such as comma, single/double quote, question marks at the start and end of each word. E.g. beautiful!!!!!! replaced with beautiful\n",
    "4. Words must start with an alphabet - For simplicity sake, we can remove all those words which don't start with an alphabet. E.g. 15th, 5.34am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('URL')\n",
    "    stopWords.append('RT')\n",
    "    stopWords.append('AT_USER')\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "\n",
    "def getFeatureVector(tweet, stwl):\n",
    "    featureVector = []\n",
    "    tweet = processTweet(tweet)\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = re.sub('[^\\w\\s]','', w)\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stwl or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test\n",
    "Test the preprocessing and features extraction functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'drug', 'olympicssomeone', 'check', 'bags', 'dope', 'judging', 'boxingrio2016', 'peetestforjudges', 'gotospecsavers']\n"
     ]
    }
   ],
   "source": [
    "stopWords = getStopWordList('/Users/nicolavitale/Desktop/twitter_data_analysis/develop/data/SmartStoplist.txt')\n",
    "# test\n",
    "tweet = \"With all the drug use going on in the olympics,someone check the 3 bags of dope judging the boxing#Rio2016 #peetestforjudges #gotospecsavers\"\n",
    "features_vector = getFeatureVector(tweet, stopWords)\n",
    "print features_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentiment_Analysis_Dataset.csv\n",
    "\n",
    "``\n",
    "ItemID,Sentiment,SentimentSource,SentimentText\n",
    "1,0,Sentiment140,                     is so sad for my APL friend.............\n",
    "2,0,Sentiment140,                   I missed the New Moon trailer...\n",
    "3,1,Sentiment140,              omg its already 7:30 :O\n",
    "4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
    "5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T\n",
    "6,0,Sentiment140,         or i just worry too much?        \n",
    "7,1,Sentiment140,       Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
    "8,0,Sentiment140,       Sunny Again        Work Tomorrow  :-|       TV Tonight\n",
    "9,1,Sentiment140,      handed in my uniform today . i miss you already\n",
    "10,1,Sentiment140,      hmmmm.... i wonder how she my number @-)\n",
    "``\n",
    "\n",
    "* Sentiment == 1, positive\n",
    "* sentiment == 0, negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 8836: expected 4 fields, saw 5\n",
      "\n",
      "Skipping line 535882: expected 4 fields, saw 7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344500\n"
     ]
    }
   ],
   "source": [
    "# Load all the CSV file in a dictionary of lisits of the form\n",
    "# sentiment_ds_p_n = {\"ItemID\":[...], \"Sentiment\":[...], \"SentimentText\":[...]}\n",
    "df_csv = pd.read_csv('/Users/nicolavitale/Desktop/twitter_data_analysis/develop/data/sentiment_analysis/Sentiment_Analysis_Dataset.csv',header=0, error_bad_lines=False)\n",
    "# df_csv.drop('SentimentSource')\n",
    "# df_csv.drop('ItemID')\n",
    "# 1578612 total classified tweets\n",
    "# separate in positive and negative\n",
    "df_pos = df_csv.loc[df_csv['Sentiment'] == 1]\n",
    "df_neg = df_csv.loc[df_csv['Sentiment'] == 0]\n",
    "# use around 600 tweets; 2/3 training (400), 1/3 test (200)\n",
    "pos_train = df_pos[0:344500]\n",
    "neg_train = df_neg[0:500000]\n",
    "train_pn = pd.concat([pos_train, neg_train])\n",
    "\n",
    "pos_test = df_pos[200001:300000]\n",
    "neg_test = df_neg[200001:300000]\n",
    "test_pn = pd.concat([pos_test, neg_test])\n",
    "\n",
    "print len(pos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200529\n",
      "207463\n"
     ]
    }
   ],
   "source": [
    "#Read the tweets one by one and process them\n",
    "stopWords = getStopWordList('/Users/nicolavitale/Desktop/twitter_data_analysis/develop/data/SmartStoplist.txt')\n",
    "# tw_sent_p = []\n",
    "twft_sent_p = []\n",
    "\n",
    "# tw_sent_n = []\n",
    "twft_sent_n = []\n",
    "\n",
    "# ftrs_list = []\n",
    "\n",
    "\n",
    "for i in range(0,len(pos_train)):\n",
    "    try:\n",
    "        tweet = pos_train['SentimentText'][i]\n",
    "        sentiment = pos_train['Sentiment'][i]\n",
    "        featureVector = getFeatureVector(tweet, stopWords)\n",
    "#         ftrs_list.extend(featureVector)\n",
    "#         tw_sent_p.append((tweet, sentiment))\n",
    "        twft_sent_p.append((featureVector, sentiment))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "        \n",
    "        \n",
    "for i in range(0,len(neg_train)):\n",
    "    try:\n",
    "        tweet = neg_train['SentimentText'][i]\n",
    "        sentiment = neg_train['Sentiment'][i]\n",
    "        featureVector = getFeatureVector(tweet, stopWords)\n",
    "#         ftrs_list.extend(featureVector)\n",
    "#         tw_sent_n.append((tweet, sentiment))\n",
    "        twft_sent_n.append((featureVector, sentiment))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "\n",
    "        \n",
    "# ftrs_list = list(set(ftrs_list))\n",
    "print len(twft_sent_p)\n",
    "print len(twft_sent_n)\n",
    "\n",
    "\n",
    "# with open('twft_sent.pickle', 'wb') as handle:\n",
    "#         pickle.dump(twft_sent, handle)\n",
    "# with open('ftrs_list.pickle', 'wb') as handle:\n",
    "#         pickle.dump(ftrs_list, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['goodsex', 'when', 'makes', 'squirt'], 1), (['jdedwards', 'link', 'jdedwards', 'live', 'gt'], 0), (['lt3', 'love', 'robbie', 'williams'], 1), (['quotupquot', 'good', 'company'], 1), (['quot', 'je', 'te', 'promets', 'quot', 'zaho', 'my', 'favourite', 'song', 'moment'], 1), (['sun', 'isnt', 'today', 'now'], 0), (['mates', 'miserable', 'git', 'lol'], 1), (['dreams', 'true'], 1), (['haha', 'dating', 'enemy'], 1), (['sick', 'might', 'swine', 'flu', 'noo', 'prob', 'docs', 'tomorrow'], 0), (['followfriday', 'sick', 'head', 'i', 'whining', 'tagged', 'fridays'], 1), (['followfriday', 'friends', 'enjoy', 'tweets', 'tgif', 'have', 'great', 'day'], 1), (['fcked', 'thing', 'good'], 0), (['gonna', 'rain', 'so', 'cal', 'thunder', 'lightning'], 1), (['month', 'middle', 'winter', 'day', 'im', 'lost', 'hurry'], 0), (['gtlt', 'ive', 'spacing', 'nightmares', 'day', 'i', 'hate', 'real', 'here', 'i'], 0), (['insomniac', 'enjoying', 'life'], 1), (['meebo', 'awesome'], 1), (['loading', 'asot400'], 0), (['govt', 'motors', 'peed', 'amp'], 1)]\n",
      "\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "twft_sent_p = twft_sent_p[0:5000]\n",
    "# print twft_sent_p[0:10]\n",
    "# print len(twft_sent_p)\n",
    "\n",
    "twft_sent_n = twft_sent_n[0:5000]\n",
    "# print twft_sent_n[0:10]\n",
    "# print len(twft_sent_n)\n",
    "\n",
    "documents = []\n",
    "documents.extend(twft_sent_p)\n",
    "documents.extend(twft_sent_n)\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "print documents[0:20]\n",
    "print\n",
    "print len(documents)\n",
    "\n",
    "# all_ftrs = []\n",
    "\n",
    "# for i in range(0,len(twft_sent)):\n",
    "#     tweet = twft_sent[i][0]\n",
    "#     train_ftrs.extend(tweet)\n",
    "    \n",
    "\n",
    "# train_ftrs = list(set(train_ftrs))\n",
    "# print train_ftrs[0:100]\n",
    "# print len(train_ftrs)\n",
    "\n",
    "\n",
    "# all_words = nltk.FreqDist(w.lower() for do in movie_reviews.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16584\n"
     ]
    }
   ],
   "source": [
    "all_ftrs = []\n",
    "\n",
    "for i in range(0,len(documents)):\n",
    "    tweet = documents[i][0]\n",
    "    all_ftrs.extend(tweet)        \n",
    "\n",
    "all_ftrs = list(set(all_ftrs))\n",
    "print len(all_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 16584 samples and 16584 outcomes>\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(i.lower() for i in all_ftrs)\n",
    "print all_words\n",
    "\n",
    "word_features = list(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[:7000], featuresets[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('twft_sent.pickle', 'rb') as handle:\n",
    "#     twft_sent = pickle.load(handle)\n",
    "# with open('ftrs_list.pickle', 'rb') as handle:\n",
    "#     featureList = pickle.load(handle)\n",
    "    \n",
    "# def extract_features(twft):\n",
    "#     tweet_words = set(twft)\n",
    "#     features = {}\n",
    "#     for word in twft:\n",
    "#         features['contains(%s)' % word] = (word in tweet_words)\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def extract_train(twft_sent):\n",
    "#     training_set = []\n",
    "#     for t_s in twft_sent:\n",
    "#         training_set.append((extract_features(t_s[0], train_ftrs), t_s[1]))\n",
    "#     return training_set\n",
    "\n",
    "# tr_st = extract_train(twft_sent[0:10])\n",
    "# print tr_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print len(train_set)\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "NBClassifier_70h = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "with open('NBClassifier_70h.pickle', 'wb') as handle:\n",
    "        pickle.dump(NBClassifier_70h, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Use the classifier saved as pickle object. (Do not return on the above code)\n",
    "\n",
    "__CLASSIFIER TRAINED AND SAVED AS PICKLE OBJECT!!!!!!!!!!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# with open('NBClassifier_70h.pickle', 'rb') as handle:\n",
    "#     NBClassifier_70h = pickle.load(handle)\n",
    "\n",
    "stwl = getStopWordList('/Users/nicolavitale/Desktop/twitter_data_analysis/develop/data/SmartStoplist.txt')\n",
    "\n",
    "# Test the classifier\n",
    "testTweet = \"The beast has arrived. Smok TFV8 review. https://t.co/VxA7bs5d8m #vapeon #vapelife #ecigs #ecig #vape #vaping https://t.co/5DhEoObNXu\"\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "# print processedTestTweet\n",
    "print NBClassifier_70h.classify(document_features(getFeatureVector(processTweet(testTweet), stwl)))\n",
    "\n",
    "\n",
    "# process the test set before !!!\n",
    "# print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(NBClassifier, test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(NBClassifier_70h, test_set))\n",
    "# try to test only on positive and only on negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(sad) = True                0 : 1      =     44.2 : 1.0\n",
      "         contains(quoti) = True                1 : 0      =     40.8 : 1.0\n",
      "   contains(musicmonday) = True                1 : 0      =     36.5 : 1.0\n",
      "  contains(followfriday) = True                1 : 0      =     21.8 : 1.0\n",
      "       contains(youquot) = True                1 : 0      =     20.6 : 1.0\n",
      "       contains(missing) = True                0 : 1      =     16.8 : 1.0\n",
      "       contains(anymore) = True                0 : 1      =     14.2 : 1.0\n",
      "         contains(hurts) = True                0 : 1      =     14.0 : 1.0\n",
      "        contains(thanks) = True                1 : 0      =     11.4 : 1.0\n",
      "          contains(poor) = True                0 : 1      =     11.3 : 1.0\n",
      "            contains(ff) = True                1 : 0      =     10.9 : 1.0\n",
      "       contains(quotthe) = True                1 : 0      =     10.5 : 1.0\n",
      "           contains(ugh) = True                0 : 1      =     10.5 : 1.0\n",
      "          contains(died) = True                0 : 1      =     10.5 : 1.0\n",
      "         contains(hates) = True                0 : 1      =     10.2 : 1.0\n",
      "         contains(enjoy) = True                1 : 0      =      9.8 : 1.0\n",
      "           contains(fml) = True                0 : 1      =      9.6 : 1.0\n",
      "        contains(follow) = True                1 : 0      =      9.5 : 1.0\n",
      "          contains(sexy) = True                1 : 0      =      9.1 : 1.0\n",
      "        contains(longer) = True                0 : 1      =      8.9 : 1.0\n",
      "       contains(worried) = True                0 : 1      =      8.9 : 1.0\n",
      "           contains(noo) = True                0 : 1      =      8.9 : 1.0\n",
      "           contains(yes) = True                1 : 0      =      8.4 : 1.0\n",
      "        contains(quotim) = True                1 : 0      =      8.3 : 1.0\n",
      "          contains(card) = True                0 : 1      =      8.2 : 1.0\n",
      "          contains(sold) = True                0 : 1      =      8.2 : 1.0\n",
      "          contains(hate) = True                0 : 1      =      8.0 : 1.0\n",
      "       contains(special) = True                1 : 0      =      7.8 : 1.0\n",
      "         contains(plane) = True                0 : 1      =      7.6 : 1.0\n",
      "          contains(ughh) = True                0 : 1      =      7.6 : 1.0\n",
      "          contains(quot) = True                1 : 0      =      7.5 : 1.0\n",
      "       contains(quotits) = True                1 : 0      =      7.5 : 1.0\n",
      "          contains(sick) = True                0 : 1      =      7.4 : 1.0\n",
      "       contains(goodsex) = True                1 : 0      =      7.3 : 1.0\n",
      "contains(andyclemmensen) = True                1 : 0      =      7.1 : 1.0\n",
      "         contains(laugh) = True                1 : 0      =      7.1 : 1.0\n",
      "  contains(shaundiviney) = True                1 : 0      =      7.1 : 1.0\n",
      "           contains(her) = True                1 : 0      =      7.1 : 1.0\n",
      "      contains(homework) = True                0 : 1      =      6.9 : 1.0\n",
      "        contains(crying) = True                0 : 1      =      6.9 : 1.0\n",
      "         contains(wrong) = True                0 : 1      =      6.9 : 1.0\n",
      "        contains(lonely) = True                0 : 1      =      6.9 : 1.0\n",
      "          contains(trip) = True                0 : 1      =      6.9 : 1.0\n",
      "       contains(airport) = True                0 : 1      =      6.9 : 1.0\n",
      "     contains(negmaremo) = True                0 : 1      =      6.8 : 1.0\n",
      "       contains(wouldnt) = True                0 : 1      =      6.5 : 1.0\n",
      "         contains(quota) = True                1 : 0      =      6.4 : 1.0\n",
      "         contains(smile) = True                1 : 0      =      6.4 : 1.0\n",
      "         contains(broke) = True                0 : 1      =      6.4 : 1.0\n",
      "           contains(mw2) = True                1 : 0      =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "NBClassifier_70h.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Apply classifier to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.sn_sp\n",
    "collection1 = db.net_00\n",
    "collection2 = db.net_1\n",
    "\n",
    "cursor = collection1.find({})\n",
    "\n",
    "items_list = [ item for item in cursor ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet_doc in items_list:\n",
    "    tweet_doc['sentiment'] = NBClassifier_70h.classify(document_features(getFeatureVector(processTweet(tweet_doc['text']), stwl)))\n",
    "    collection2.insert_one(tweet_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
