{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Twitter API for social research purposes\n",
    "Setting objectives (just an example, pipeline has been modified):\n",
    "1. Start from a basic ashtag on smoke \n",
    "    * use _GET search/tweets_ methods in the __REST API__\n",
    "    * start pulling data into MongoDB using same ashtags and __stresming API__\n",
    "2. Look for co-occurrence of ashstags (count other ashtags that appear the most with the searched ones)\n",
    "3. Start \"tracking\" also those ashstags --> list of ashstags\n",
    "4. Find the ashstags that are more correlated with smoke\n",
    "5. Network of ashstags\n",
    "(Repeat the process using keywords instead of ashtags.\n",
    "Eventually we could use a mixed approach)\n",
    "\n",
    "#### Empirical research on twitter.com \n",
    "**_#tobacco_** is in the first two suggestions when we start the search.\n",
    "Could be a good starting point as it seems to have a more neutral sentiment compared to other choiches that appear (e.g. _#tobaccofree_, _#smoke_, _#smokefree_; hopefully we will find these ashtag in the network)\n",
    "\n",
    "\n",
    "# REST and streaming API\n",
    "We obtain the same objects (tweets) from both APIs but different ones hopefully.\n",
    "In MongoDB create a collection for each: -search -stream\n",
    "\n",
    "#### REST API\n",
    "The _GET search/tweets_ has __number of queries over time limit__.\n",
    "We have to use it wisely and be careful of not collecting the same tweet twice (see indexing).\n",
    "\n",
    "WHAT ARE WE USING HERE?\n",
    "\n",
    "- API type\n",
    "[REST APIs](https://dev.twitter.com/rest/public)\n",
    "\n",
    "- API sub-type\n",
    "[The Search API](https://dev.twitter.com/rest/public/search)\n",
    "\n",
    "- Method of the API\n",
    "[GET search/tweets](https://dev.twitter.com/rest/reference/get/search/tweets)\n",
    "\n",
    "- Parameters callable on this method (parameters are similar for all methods within API type)\n",
    "[Search parameters](https://dev.twitter.com/rest/reference/get/search/tweets)\n",
    "\n",
    "__Note__: Unlike the streaming API which has the same parameters on all its methods (that are way less), the parameters of the REST API methods are reported per each method, because they vary.\n",
    "\n",
    "Rate limits for this API explained:\n",
    "[How do rate limits work for the REST API? (Stack Overflow)](http://stackoverflow.com/questions/21305547/how-rate-limit-works-in-twitter-in-search-api)\n",
    "\n",
    "#### Streaming API\n",
    "Start streaming tweet into Mongo and update the ashtags list (key-words list) as new evidence is found.\n",
    "\n",
    "WHAT ARE WE USING HERE?\n",
    "\n",
    "- API type\n",
    "[The Streaming APIs](https://dev.twitter.com/streaming/overview)\n",
    "\n",
    "- API sub-type\n",
    "[Public streams](https://dev.twitter.com/streaming/public)\n",
    "\n",
    "- Method of the API\n",
    "[GET statuses/sample](https://dev.twitter.com/streaming/reference/get/statuses/sample)\n",
    "\n",
    "- Parameters callable on this method (parameters are similar for all methods within API type)\n",
    "[Statuses parameters](https://dev.twitter.com/streaming/overview/request-parameters)\n",
    "\n",
    "__Note__: The streaming API has common parameters that can be called on each of its methods (follow the last link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Twitter for any API requests\n",
    "Application created \"soton_uni\" [Application Management](https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x10a0311d0>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "def oauth_login():\n",
    "    \n",
    "    CONSUMER_KEY = '6YyEfFZtKh3qoGJ3DTy35ToFl'\n",
    "    CONSUMER_SECRET = 'dvhZX8j3kp5sPcDNivj8BGLoylJUOUbQkVG3qbICNA81R86kh8'\n",
    "    OAUTH_TOKEN = '4061210361-6OWiTmHf6JpMBdjWnk3GHzNo57M1AtAXxF1gxdt'\n",
    "    OAUTH_TOKEN_SECRET = 'QwvuefhvzzSFbuECijEyj1hPMA2jelF1sdpFD7hDmhZZl'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# test\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "# twitter_api it's now a defined variable\n",
    "print twitter_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use GET search/tweets method in the REST API\n",
    "Search for the **_#tobacoo_** ashtag. We use it as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of statuses 100\n",
      "Length of statuses 200\n",
      "Length of statuses 300\n",
      "Length of statuses 400\n",
      "Length of statuses 500\n",
      "Length of statuses 600\n",
      "Length of statuses 700\n",
      "Length of statuses 800\n",
      "Length of statuses 900\n",
      "Length of statuses 1000\n",
      "Length of statuses 1100\n",
      "Length of statuses 1200\n",
      "Length of statuses 1300\n",
      "Length of statuses 1400\n",
      "Length of statuses 1500\n",
      "Length of statuses 1600\n",
      "Length of statuses 1693\n",
      "1693\n",
      "\n",
      "\n",
      "\"RT @BBCSport: Olympic and Wimbledon champion Andy Murray will carry the flag for @TeamGB at #Rio2016 \\ud83c\\uddec\\ud83c\\udde7\\n\\nhttps://t.co/z7SBWIWP93 https://t.\\u2026\"\n",
      "\n",
      "\"RT @_dogandbone_: #RETWEET #FOLLOW @LilEddies 4 a chance to #WIN a PHONECASE #BIGSALE #SUMMER #WednesdayWisdom https://t.co/8JayfDruVY http\\u2026\"\n",
      "\n",
      "\"#LFC Star https://t.co/lbVIbqdMfe Liverpool ace: This England star is an inspiration to me\"\n",
      "\n",
      "\"RT @AutoExpress: New #RAC report reveals pothole damage to UK cars has doubled in a decade... https://t.co/u1vB5GHoNp https://t.co/whEPsMjR\\u2026\"\n",
      "\n",
      "\"Interview: @jaredyung of UK  https://t.co/raNX7WSnfe @cassidy_larsiny @minarmy\"\n",
      "\n",
      "\"RT @itsmattwilmott: Suicide Squad fans petition to shut down Rotten Tomatoes following bad reviews https://t.co/BaGxHcmz64 this is sad on s\\u2026\"\n",
      "\n",
      "\"RT @swingaleg: Like Jon, I'm actually amazed the entire country never starved to death prior to the UK joining the EU. https://t.co/AOn3A5C\\u2026\"\n",
      "\n",
      "\"RT @MissAmyVarle: We make a living by what we get, but we make a life by what we give. #Churchill2016 #quotes https://t.co/I1YkPVqfQw https\\u2026\"\n",
      "\n",
      "\"RT @TennisNewsTPN: Andy Murray set to be Team GB Olympic opening ceremony flag-bearer https://t.co/jv2aGvx9YB v\"\n",
      "\n",
      "\"Lake District, UK [OC] [1087x720] via /r/EarthPorn https://t.co/aNxGixexnj https://t.co/XXqTD4CkSv\"\n",
      "\n",
      "\"RT @FootBoiller: How Liverpool FC have made Klopp feel even more at home in the ... - Liverpool Echo https://t.co/tG0f4T0BVE #LFC\"\n",
      "\n",
      "\"RT @Mark_Beech: Top 8 Shows at Edinburgh Fringe 2016 @edfringe @edinburghfest | BLOUIN ARTINFO... https://t.co/1YlcAGBumh\"\n",
      "\n",
      "\"RT @PercyCuteTees: This is how we #sketch https://t.co/fNm8wuqDvT #streetwear take a look at what we do ! https://t.co/aLkCRkdvle\"\n",
      "\n",
      "\"\\u3069\\u3046\\u3084\\u3063\\u3066\\u6c7a\\u3081\\u305f\\u304b\\u805e\\u304d\\u305f\\u3044 https://t.co/0QD5fFHcxP\"\n",
      "\n",
      "\"RT @DailyMirror: ISIS training base hidden in Saddam Hussein's former palace\\nhttps://t.co/9fAZtztwv8 https://t.co/OUzCJjTrmt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def twitter_search(twitter_api, q, max_results=100000, **kw):\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results. Maximum count is 100, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval, reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(18000, max_results)\n",
    "    \n",
    "    for _ in range(179): # 180 max number of query we can issue in 15 minutes (1 it is outside the loop)\n",
    "        print \"Length of statuses\", len(statuses)\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError, e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "\n",
    "        statuses.extend(search_results['statuses'])\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "        \n",
    "    return statuses\n",
    "\n",
    "\n",
    "# test\n",
    "twitter_api = oauth_login()\n",
    "q = \"uk\"\n",
    "\n",
    "results = twitter_search(twitter_api, q)\n",
    "\n",
    "# Number of tweets collected with the query\n",
    "print len(results)\n",
    "print\n",
    "print\n",
    "\n",
    "# Show 15 tweets: empirical exploration\n",
    "for i in range(15):\n",
    "    print json.dumps(results[i][\"text\"], indent=1)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__:\n",
    "Although we're just passing in a hashtag to the Search API at this point, it's well worth noting that it contains a number of [powerful operators](https://dev.twitter.com/rest/public/search) that allow you to filter queries according to the existence or nonexistence of various keywords, originator of the tweet, location associated with the tweet, etc.\n",
    "\n",
    "__Note:__ when we issue a serch query to the REST API what we are actually returned is the following object; the result of our query in terms of tweets is inside _statuses_ but we also have search metadata that we use to continue our search and return more than 100 tweets (query limit).\n",
    "    \n",
    "--------------------------------------------------------------------------------------------------------------    \n",
    "    {\n",
    "    \n",
    "    \"search_metadata\": {\n",
    "        \"completed_in\": 0.092, \n",
    "        \"count\": 100, \n",
    "        \"max_id\": 749828837471584259, \n",
    "        \"max_id_str\": \"749828837471584259\", \n",
    "        \"next_results\": \"?max_id=749671693128531967&q=%2523tobacco&count=100&include_entities=1\", \n",
    "        \"query\": \"%2523tobacco\", \n",
    "        \"refresh_url\": \"?since_id=749828837471584259&q=%2523tobacco&include_entities=1\", \n",
    "        \"since_id\": 0, \n",
    "        \"since_id_str\": \"0\"\n",
    "    }, \n",
    "    \n",
    "    \"statuses\": [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        ...\n",
    "        ## tweets ##\n",
    "        ...\n",
    "        {},\n",
    "        {},\n",
    "        {}\n",
    "        ]\n",
    "        \n",
    "     }\n",
    "     \n",
    "--------------------------------------------------------------------------------------------------------------     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The search_metadata and the cursoring system\n",
    "\n",
    "In essence, all the previous code for the search does is __repeatedly make requests to the Search API__(enforcing a limit on the number of request not to pass the Twitter limits). \n",
    "One thing that might initially catch you off guard if you've worked with other web APIs (including version 1 of Twitter's API) is that there's no explicit concept of pagination in the Search API itself. Reviewing the API documentation reveals that this is a intentional decision, and there are some good reasons for taking a [cursoring approach](https://dev.twitter.com/rest/public/timelines) instead, given the highly dynamic state of Twitter resources. The best practices for cursoring vary a bit throughout the Twitter developer platform, with the Search API providing a slightly simpler way of navigating search results than other resources such as timelines.\n",
    "Search results contain a special __search_metadata__ node that embeds a __next_results field__ with __a query string that provides the basis of a subsequent query__. If we weren't using a library like twitter to make the HTTP requests for us, this preconstructed query string would just be appended to the Search API URL, and we'd update it with additional parameters for handling OAuth. However, __since we are not making our HTTP requests directly, we must parse the query string into its constituent key/value pairs and provide them as keyword arguments.__\n",
    "\n",
    "In Python parlance, we are unpacking the values in a dictionary into keyword arguments that the API search function receives (twitter_api.search.tweets(__**kwargs__)). In other words, the function call inside of the for loop ultimately invokes a function such as twitter_api.search.tweets(q='%23Tobacco', include_entities=1, max_id=313519052523986943) even though it appears in the source code as twitter_api.search.tweets(__**kwargs__), with kwargs being a dictionary of key/value pairs ([*Args, **Kwargs](https://www.youtube.com/watch?v=WWm5DxTzLuk)).\n",
    "\n",
    "__Note__:\n",
    "The search_metadata field also contains a __refresh_url__ value that can be used if you'd like to maintain and periodically update your collection of results with new information that's become available since the previous query.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In this example we use the GET search/tweets method of the Search API to perform a custom query  against the entire twitterverse. Similar to the way that search engines work, __Twitter's Search API return results in batches, and you can configure the number of results per batch to a maximum value of 100 by using the count keyword (count: The number of tweets to return per page, up to a maximum of 100. Defaults to 15. This was formerly the “rpp” parameter in the old Search API; from documentation).__ It is possible that more than 100 results (or the count specified) may be available for a given query and, in the parlance of Twitter's API, you'll need to use a  **_cursor_**, to __navigate to the next batch of results__.\n",
    "\n",
    "[Cursors](https://dev.twitter.com/rest/public/timelines) are a new enhancement to Twitter's v1.1 API and provide a more robust scheme than the pagination paradigm offered by the v1.0 API, which involved specifying a page number and a result per page constraint. The essence of the cursor paradigm is that it is able to better accomodate the dynamic and real-time nature of the Twitter platform. For example, Twitter's API cursor are designed to inherently take into account the possibility that updated information may become available in real time while you are navigating a batch of query results. In other words, it could be the case that while you are navigating a batch of query resuts, relevant information may become available that you would want to have included in your current results while you are navigating them, rather then needing to dispatch a new query.\n",
    "\n",
    "With the code in the previous function, we use the Search API and navigate the cursor that is included in a response (in the search_metadata field) to fetch more than one batch of results.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# use same query within streaming API\n",
    "Search for the **_#tobacoo_** ashtag.\n",
    "\n",
    "__Note:__Check the parameters of the stream method [Streaming API request parameters](https://dev.twitter.com/streaming/overview/request-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering the public timeline for track=\"tobacco\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object __iter__ at 0x10cf27f50>\n",
      "{u'favorited': False, u'contributors': None, u'truncated': False, u'text': u'RT @mihotep: #harmless #vaping\\n\\u2705eliminates the #Tobacco\\n\\u2705*ENDS* the Con\\nin #TobaccoConTrol\\n\\n#WorldLungCancerDay https://t.co/y7o0iDDEQg', u'possibly_sensitive': False, u'is_quote_status': False, u'in_reply_to_status_id': None, u'user': {u'follow_request_sent': None, u'profile_use_background_image': False, u'default_profile_image': False, u'id': 119625872, u'verified': False, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/583067608838094849/IdqLtH7G_normal.jpg', u'profile_sidebar_fill_color': u'000000', u'profile_text_color': u'000000', u'followers_count': 382, u'profile_sidebar_border_color': u'000000', u'id_str': u'119625872', u'profile_background_color': u'000000', u'listed_count': 40, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme9/bg.gif', u'utc_offset': -14400, u'statuses_count': 4827, u'description': u'Thx to vaping, SMOKE-FREE FOR 2 YRS! -- after 39 yrs of smoking. FDA, get out of bed with Big Pharma, you greedy, evil liars!', u'friends_count': 288, u'location': u'Atlanta, GA', u'profile_link_color': u'9266CC', u'profile_image_url': u'http://pbs.twimg.com/profile_images/583067608838094849/IdqLtH7G_normal.jpg', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/119625872/1427865417', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme9/bg.gif', u'name': u'Andria Duncan', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 3525, u'screen_name': u'CarmAndria', u'notifications': None, u'url': u'http://angryvaper.crypticsites.com', u'created_at': u'Thu Mar 04 05:09:26 +0000 2010', u'contributors_enabled': False, u'time_zone': u'Eastern Time (US & Canada)', u'protected': False, u'default_profile': False, u'is_translator': False}, u'filter_level': u'low', u'geo': None, u'id': 760524981587570688, u'favorite_count': 0, u'lang': u'en', u'retweeted_status': {u'contributors': None, u'truncated': False, u'text': u'#harmless #vaping\\n\\u2705eliminates the #Tobacco\\n\\u2705*ENDS* the Con\\nin #TobaccoConTrol\\n\\n#WorldLungCancerDay https://t.co/y7o0iDDEQg', u'is_quote_status': False, u'in_reply_to_status_id': None, u'id': 760213001089589248, u'favorite_count': 2, u'source': u'<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', u'retweeted': False, u'coordinates': None, u'entities': {u'user_mentions': [], u'symbols': [], u'hashtags': [{u'indices': [0, 9], u'text': u'harmless'}, {u'indices': [10, 17], u'text': u'vaping'}, {u'indices': [34, 42], u'text': u'Tobacco'}, {u'indices': [62, 77], u'text': u'TobaccoConTrol'}, {u'indices': [79, 98], u'text': u'WorldLungCancerDay'}], u'urls': [], u'media': [{u'expanded_url': u'http://twitter.com/mihotep/status/760213001089589248/photo/1', u'display_url': u'pic.twitter.com/y7o0iDDEQg', u'url': u'https://t.co/y7o0iDDEQg', u'media_url_https': u'https://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg', u'id_str': u'760212997750927360', u'sizes': {u'small': {u'h': 366, u'resize': u'fit', u'w': 616}, u'large': {u'h': 366, u'resize': u'fit', u'w': 616}, u'medium': {u'h': 366, u'resize': u'fit', u'w': 616}, u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}}, u'indices': [99, 122], u'type': u'photo', u'id': 760212997750927360, u'media_url': u'http://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg'}]}, u'in_reply_to_screen_name': None, u'id_str': u'760213001089589248', u'retweet_count': 4, u'in_reply_to_user_id': None, u'favorited': False, u'user': {u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 2910321650, u'verified': False, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/655148940426252288/GUwmqGGL_normal.png', u'profile_sidebar_fill_color': u'95E8EC', u'profile_text_color': u'3C3940', u'followers_count': 2723, u'profile_sidebar_border_color': u'5ED4DC', u'id_str': u'2910321650', u'profile_background_color': u'0099B9', u'listed_count': 215, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme4/bg.gif', u'utc_offset': None, u'statuses_count': 47817, u'description': u\"Quit #smoking with #ecigs. #Advocate harm-reduction. Save #ABillionLives. Deflate nannycrat pseudoscience masquerading as ''#publichealth''-#vapingtruth\", u'friends_count': 693, u'location': u'Unpaid Vaper and Advocate', u'profile_link_color': u'0099B9', u'profile_image_url': u'http://pbs.twimg.com/profile_images/655148940426252288/GUwmqGGL_normal.png', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/2910321650/1446061499', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme4/bg.gif', u'name': u'DrMA', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 85921, u'screen_name': u'mihotep', u'notifications': None, u'url': u'https://www.gov.uk/government/publications/e-cigarettes-an-evidence-update', u'created_at': u'Tue Nov 25 20:35:47 +0000 2014', u'contributors_enabled': False, u'time_zone': None, u'protected': False, u'default_profile': False, u'is_translator': False}, u'geo': None, u'in_reply_to_user_id_str': None, u'possibly_sensitive': False, u'lang': u'en', u'created_at': u'Mon Aug 01 20:37:58 +0000 2016', u'filter_level': u'low', u'in_reply_to_status_id_str': None, u'place': None, u'extended_entities': {u'media': [{u'expanded_url': u'http://twitter.com/mihotep/status/760213001089589248/photo/1', u'display_url': u'pic.twitter.com/y7o0iDDEQg', u'url': u'https://t.co/y7o0iDDEQg', u'media_url_https': u'https://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg', u'id_str': u'760212997750927360', u'sizes': {u'small': {u'h': 366, u'resize': u'fit', u'w': 616}, u'large': {u'h': 366, u'resize': u'fit', u'w': 616}, u'medium': {u'h': 366, u'resize': u'fit', u'w': 616}, u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}}, u'indices': [99, 122], u'type': u'photo', u'id': 760212997750927360, u'media_url': u'http://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg'}]}}, u'entities': {u'user_mentions': [{u'id': 2910321650, u'indices': [3, 11], u'id_str': u'2910321650', u'screen_name': u'mihotep', u'name': u'DrMA'}], u'symbols': [], u'hashtags': [{u'indices': [13, 22], u'text': u'harmless'}, {u'indices': [23, 30], u'text': u'vaping'}, {u'indices': [47, 55], u'text': u'Tobacco'}, {u'indices': [75, 90], u'text': u'TobaccoConTrol'}, {u'indices': [92, 111], u'text': u'WorldLungCancerDay'}], u'urls': [], u'media': [{u'source_user_id': 2910321650, u'source_status_id_str': u'760213001089589248', u'expanded_url': u'http://twitter.com/mihotep/status/760213001089589248/photo/1', u'display_url': u'pic.twitter.com/y7o0iDDEQg', u'url': u'https://t.co/y7o0iDDEQg', u'media_url_https': u'https://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg', u'source_user_id_str': u'2910321650', u'source_status_id': 760213001089589248, u'id_str': u'760212997750927360', u'sizes': {u'small': {u'h': 366, u'resize': u'fit', u'w': 616}, u'large': {u'h': 366, u'resize': u'fit', u'w': 616}, u'medium': {u'h': 366, u'resize': u'fit', u'w': 616}, u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}}, u'indices': [112, 135], u'type': u'photo', u'id': 760212997750927360, u'media_url': u'http://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg'}]}, u'in_reply_to_user_id_str': None, u'retweeted': False, u'coordinates': None, u'timestamp_ms': u'1470158260138', u'source': u'<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', u'in_reply_to_status_id_str': None, u'in_reply_to_screen_name': None, u'id_str': u'760524981587570688', u'extended_entities': {u'media': [{u'source_user_id': 2910321650, u'source_status_id_str': u'760213001089589248', u'expanded_url': u'http://twitter.com/mihotep/status/760213001089589248/photo/1', u'display_url': u'pic.twitter.com/y7o0iDDEQg', u'url': u'https://t.co/y7o0iDDEQg', u'media_url_https': u'https://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg', u'source_user_id_str': u'2910321650', u'source_status_id': 760213001089589248, u'id_str': u'760212997750927360', u'sizes': {u'small': {u'h': 366, u'resize': u'fit', u'w': 616}, u'large': {u'h': 366, u'resize': u'fit', u'w': 616}, u'medium': {u'h': 366, u'resize': u'fit', u'w': 616}, u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}}, u'indices': [112, 135], u'type': u'photo', u'id': 760212997750927360, u'media_url': u'http://pbs.twimg.com/media/CozRsZSUMAAf4PG.jpg'}]}, u'place': None, u'retweet_count': 0, u'created_at': u'Tue Aug 02 17:17:40 +0000 2016', u'in_reply_to_user_id': None}\n",
      "{u'favorited': False, u'contributors': None, u'truncated': False, u'text': u'\\u30de\\u30eb\\u30dc\\u30ed\\u30fb\\u30e1\\u30d3\\u30a6\\u30b9\\u30fb\\u30ad\\u30e3\\u30e1\\u30eb\\u7b49204\\u5186\\uff5e\\uff01\\uff01https://t.co/niLZbq2ryy #\\u305f\\u3070\\u3053\\u3000#\\u30de\\u30eb\\u30dc\\u30ed\\u3000#\\u30e1\\u30d3\\u30a6\\u30b9 #\\u5b89\\u58f2\\u308a #\\u6fc0\\u5b89 #\\u30bb\\u30fc\\u30eb #\\u7279\\u4fa1', u'possibly_sensitive': False, u'is_quote_status': False, u'in_reply_to_status_id': None, u'user': {u'follow_request_sent': None, u'profile_use_background_image': False, u'default_profile_image': False, u'id': 4163442978, u'verified': False, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/673138787338162176/ZXxde4MN_normal.png', u'profile_sidebar_fill_color': u'000000', u'profile_text_color': u'000000', u'followers_count': 4756, u'profile_sidebar_border_color': u'000000', u'id_str': u'4163442978', u'profile_background_color': u'000000', u'listed_count': 9, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'utc_offset': -25200, u'statuses_count': 8762, u'description': u'\\u6d77\\u5916\\u306e\\u683c\\u5b89\\u305f\\u3070\\u3053\\u304c\\u65e5\\u672c\\u304b\\u3089\\u8cfc\\u5165\\u3067\\u304d\\u307e\\u3059\\u3002 \\u901a\\u8ca9\\u30fb\\u500b\\u4eba\\u8f38\\u5165\\u4ee3\\u884c\\u30b5\\u30dd\\u30fc\\u30c8\\u3002 \\u901a\\u8ca9\\u540c\\u69d8\\u306b\\u30aa\\u30fc\\u30c0\\u30fc\\u3059\\u308b\\u3060\\u3051\\u3067\\u30de\\u30eb\\u30dc\\u30ed\\u30fb\\u30e1\\u30d3\\u30a6\\u30b9\\u7b49248\\u5186\\uff5e\\uff01\\uff01  10\\u65e5\\u524d\\u5f8c\\u3067\\u3054\\u81ea\\u5b85\\u306b\\u30bf\\u30d0\\u30b3\\u304c\\u5c4a\\u304d\\u307e\\u3059\\u3002  \\u30ab\\u30f3\\u30dc\\u30b8\\u30a2\\u3000\\u30a2\\u30f3\\u30b3\\u30fc\\u30eb\\u30ef\\u30c3\\u30c8\\u306e\\u3042\\u308b\\u8857\\u30b7\\u30a7\\u30e0\\u30ea\\u30a2\\u30c3\\u30d7\\u3088\\u308a\\u7686\\u69d8\\u306b\\u683c\\u5b89\\u305f\\u3070\\u3053\\u3092\\u304a\\u5c4a\\u3051\\u3057\\u307e\\u3059\\u3002', u'friends_count': 4739, u'location': None, u'profile_link_color': u'000000', u'profile_image_url': u'http://pbs.twimg.com/profile_images/673138787338162176/ZXxde4MN_normal.png', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/4163442978/1449323802', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'name': u'\\u6fc0\\u5b89\\u305f\\u3070\\u3053\\u306e\\u30a2\\u30f3\\u30b3\\u30fc\\u30eb\\u30bf\\u30d0\\u30b3\\u3000\\u76f8\\u4e92\\u30d5\\u30a9\\u30ed', u'lang': u'ja', u'profile_background_tile': False, u'favourites_count': 100, u'screen_name': u'Angkor_Tobacco', u'notifications': None, u'url': u'http://angkor-tobacco.com', u'created_at': u'Sun Nov 08 02:31:58 +0000 2015', u'contributors_enabled': False, u'time_zone': u'Pacific Time (US & Canada)', u'protected': False, u'default_profile': False, u'is_translator': False}, u'filter_level': u'low', u'geo': None, u'id': 760525028643385344, u'favorite_count': 0, u'lang': u'ja', u'entities': {u'user_mentions': [], u'symbols': [], u'hashtags': [{u'indices': [46, 50], u'text': u'\\u305f\\u3070\\u3053'}, {u'indices': [51, 56], u'text': u'\\u30de\\u30eb\\u30dc\\u30ed'}, {u'indices': [57, 62], u'text': u'\\u30e1\\u30d3\\u30a6\\u30b9'}, {u'indices': [63, 67], u'text': u'\\u5b89\\u58f2\\u308a'}, {u'indices': [68, 71], u'text': u'\\u6fc0\\u5b89'}, {u'indices': [72, 76], u'text': u'\\u30bb\\u30fc\\u30eb'}, {u'indices': [77, 80], u'text': u'\\u7279\\u4fa1'}], u'urls': [{u'url': u'https://t.co/niLZbq2ryy', u'indices': [22, 45], u'expanded_url': u'http://angkor-tobacco.com', u'display_url': u'angkor-tobacco.com'}]}, u'in_reply_to_user_id_str': None, u'retweeted': False, u'coordinates': None, u'timestamp_ms': u'1470158271357', u'source': u'<a href=\"https://twitter.com/angkor_tobacco\" rel=\"nofollow\">makeangkor</a>', u'in_reply_to_status_id_str': None, u'in_reply_to_screen_name': None, u'id_str': u'760525028643385344', u'place': None, u'retweet_count': 0, u'created_at': u'Tue Aug 02 17:17:51 +0000 2016', u'in_reply_to_user_id': None}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4490e79eb78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/twitter/stream.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Decode all the things:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mdechunked_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_of_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0municode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutf8_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdechunked_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/twitter/stream.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mready_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mready_to_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finding topics of interest by using the filtering capablities it offers.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Query terms\n",
    "\n",
    "q = 'tobacco' # Comma-separated list of terms\n",
    "\n",
    "print >> sys.stderr, 'Filtering the public timeline for track=\"%s\"' % (q,)\n",
    "\n",
    "# Returns an instance of twitter.Twitter\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# See https://dev.twitter.com/docs/streaming-apis\n",
    "stream = twitter_stream.statuses.filter(track=q)\n",
    "\n",
    "# For illustrative purposes, when all else fails, search for Justin Bieber\n",
    "# and something is sure to turn up (at least, on Twitter)\n",
    "\n",
    "print stream \n",
    "\n",
    "for tweet in stream:\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__IMPORTANT Note__: when we access twitter streming API we obtain objects of type __tweet__. Read streaming API documentation here [STREAMING API](https://dev.twitter.com/streaming/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search API VS Streaming API\n",
    "The Twitter Search API is part of Twitter’s REST API. It allows queries against the indices of recent or popular Tweets and behaves similarly to, but not exactly like the Search feature available in Twitter mobile or web clients, such as Twitter.com search. __The Twitter Search API searches against a sampling of recent Tweets published in the past 7 days.__\n",
    "\n",
    "Before getting involved, it’s important to know that __the Search API is focused on relevance and not completeness.__ This means that some Tweets and users may be missing from search results. __If you want to match for completeness you should consider using a Streaming API instead.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the documents types (objects) returned by the methods we used\n",
    "\n",
    "__Note:__ all documents returned by Twitter APIs are in JSON (Java Script Object Notation) format; information is concatenated trough dictionaries {} (characterised by key-value pairs) and lists [] (ordered structure). These two basic data structures can be nested in one another.\n",
    "\n",
    "##### GET search/tweets\n",
    "\n",
    "Return a dictionary containing \"search_metadata\" and \"statuses\" keys; the actual tweets are stored in statuses (a list of tweets objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GET statuses/sample\n",
    "\n",
    "Return a \"<generator object __iter__ at 0x10cf27f50>\" object which is a kind of iterable variable updated in real time with the last streamed tweet object. The tweets can be accessed doing: \n",
    "\n",
    "for tweet in stream:\n",
    "    print tweet\n",
    "    # or do something else with the tweet like saving it in MongoDB\n",
    "\n",
    "A __tweet JSON object__ contain the text of the tweet along with many other meta-data about it (look the following example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _GET search/tweets_ method clarification\n",
    "## How to query using the Search API (GET search/tweets method)\n",
    "\n",
    "To understand the cursoring system we execute a sample query using the __GET search/tweets__ method. \n",
    "Any method that works with a cursoring system follows the same principle.\n",
    "\n",
    "Consider the following code to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "\n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    print 'search_rsults out of loop'\n",
    "    print json.dumps(search_results, indent=1)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    print 'statuses out of loop'\n",
    "    print json.dumps(statuses, indent=1)\n",
    "\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. \n",
    "    # A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "            print 'key-value pairs used to recall the function'\n",
    "            print json.dumps(next_results, indent=1)\n",
    "        except KeyError, e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        print '### ### ###'\n",
    "        print 'kwargs'\n",
    "    \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        print '### ### ###'\n",
    "        print 'search_results'\n",
    "    \n",
    "        statuses.extend(search_results['statuses']) \n",
    "        print '### ### ###'\n",
    "        print 'statuses'\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "    \n",
    "statuses_final = twitter_search(twitter_api, 'tobacco')\n",
    "print '### ### ###'\n",
    "print 'statuses_final'\n",
    "print json.dumps(statuses_final, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explain the above snippet: \n",
    "\n",
    "__We initially give a general overview of the steps it goes through.__ \n",
    "\n",
    "__Then we examine each of those reporting the output of the steps.__\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "## General overview\n",
    "\n",
    "INPUT: \n",
    "- twitter_api: instance of a twitter_api class that establish the connection with Twitter \n",
    "- q: the query (maybe possible to search for more ashtags or keywords at the same time [Using GET search/tweets to query multiple hashtags](https://twittercommunity.com/t/using-get-search-tweets-to-query-multiple-hashtags/12830))\n",
    "- max_results: maximum number of tweets we want\n",
    "- ****kv: the other query parameters given as key-value pairs (parameters of the API method, see documentation) \n",
    "\n",
    "OUTPUT:\n",
    "- statuses: list of tweet objects returned initially with search_metadata\n",
    "\n",
    "## Steps\n",
    "\n",
    "### first query to the REST API\n",
    "We do our first query to the API. This takes the __query__, the __count__ (how many tweets we want to be returned by query; the maximum allowed is 100) and other __method specific arguments__ (in this case we are using the [GET search/tweets](https://dev.twitter.com/rest/reference/get/search/tweets) method of the REST API.\n",
    "\n",
    "__Note:__ the limit of _query per time interval_ is the same for the majority of the method of the REST API.\n",
    "\n",
    "'search_results out of loop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statuses_out_of_loop\n",
    "It is the list of tweets returned by the first \"direct\" query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce a reasonable limit\n",
    "\n",
    "We build the function so that we can set the max number of tweets that the whole multi-query process can return with the initial __max_results__, but then inside the function we force that max_results to be 1000 at maximum.\n",
    "Twitter limits allow us to do 180 query per 15 minutes interval; we could theoretically force our max_results to 180x100=18000 tweets. \n",
    "\n",
    "We set a more cautious limit because:\n",
    "- the twitter search does not return tweets older than 1 week from the day the query is issued;\n",
    "- there might not be enough tweets that matches our query (if it is to specific) in the past week.\n",
    "\n",
    "__Note__: another conststraint is imposed on the lenght of the query that can be 500 characters maximum, including operators.\n",
    "\n",
    "So for example, if we query for \"#tobacco\" (a very specific string) is unlikely that we obtain more than 1000 tweets, considering that the search only goes back one week.\n",
    "\n",
    "As general benchmark consider that querying \"uk\" (a very general term) we obtain 2699 tweets.\n",
    "\n",
    "\n",
    "### Build key-value pairs from search_metadata information and issue new query\n",
    "\n",
    "When we do\n",
    "\n",
    "```python\n",
    "for _ in range(179):\n",
    "```\n",
    "we are doing at maximum 179 other queries (+1, the one outside the loop) where we build our arguments from the search_metadata information.\n",
    "In particular we are automatically sending a __max_id__ parameter so that the next query will return \"results with an ID less than (that is, older than) or equal to the specified ID\" (from documentation).\n",
    "\n",
    "\n",
    "The next_results key inside search_metadata looks like\n",
    "```python\n",
    "next_results: ?max_id=757661781355626495&q=tobacco&count=100&include_entities=1\n",
    "```\n",
    "that's why we do\n",
    "```python\n",
    "kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "```                        \n",
    "to build the dictionary that follows and pass its key-value pairs as parameters to the GET search/tweets method through\n",
    "```python\n",
    "search_results = twitter_api.search.tweets(__**kwargs__)\n",
    "```\n",
    "we finally extend our list of tweets with the new tweets by doing\n",
    "```python\n",
    "statuses.extend(search_results['statuses'])\n",
    "```\n",
    "__Note:__ in subsequent iterations the max_id parameter may remain the same; this happens because there are more than 100 tweets (the maximum that can be returned by a query) up to that id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the results is obtained through subsequent extensions of the search_results['statuses'] list. In the following case we retrived a total of 300 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "### Final note\n",
    "This method is ment to be complete for what regards the qury use but allows us to go back in time at maximum one week.\n",
    "If we identify interesting users for whom we want to collect more tweets we can use the \n",
    "\n",
    "__GET statuses/user_timeline__\n",
    "\n",
    "that allows us to take up to the last 3200 tweets of that user !!!!!!!!!\n",
    "\n",
    "### List of useful methods\n",
    "- __REST APIs__:\n",
    "    1. method with link\n",
    "        * brief description\n",
    "    2. method with link\n",
    "        * brief description\n",
    "\n",
    "\n",
    "- __The Streaming APIs__:\n",
    "    1. method with link\n",
    "        * brief description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
