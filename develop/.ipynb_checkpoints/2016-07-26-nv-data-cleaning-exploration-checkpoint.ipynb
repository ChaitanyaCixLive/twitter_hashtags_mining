{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and exploring a collection of tweets\n",
    "We go to explore the keywords_list collection in MongoDB.\n",
    "To the current date the collection has been enriched with streamed tweets containing the keyword \"tobacco\" for some days.\n",
    "\n",
    "To explore the data set we need:\n",
    "1. functions to interface with MongoDB\n",
    "    - query data\n",
    "    - load data to\n",
    "    \n",
    "2. functions to explore the data\n",
    "\n",
    "3. functions to manipualate the data\n",
    "    - cleaning \n",
    "    - features engineering\n",
    "\n",
    "We approch the problem by splitting that into 2 sections:\n",
    "- __Define the functions__\n",
    "- __Use the functions__\n",
    "\n",
    "__Note:__ When we query mongo we obtain a cursor object that contain all the documents that matched the query; to work with them we just need to do _FOR document IN cursor_\n",
    "\n",
    "Since we work with tweet object documents it is important to have always in mind the __anathomy of a tweet__; we use the following online json parser to show our documents in aplain way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"1000\" src=\"http://www.jsoneditoronline.org/\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import pymongo\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_from_mongo()\n",
    "INPUT:\n",
    "- __mongo_db__: database in which we have the cllection we are interested in (string)\n",
    "- __mongo_db_coll__: collection in which we have the documents we are interested in (string)\n",
    "- __return_cursor__: if set to TRUE return a cursor object which is a list of the documents that match our query\n",
    "- __criteria__: it is the query icluded in {} parenthesis an written in javascript syntax (same as Monngo Shell)\n",
    "- __projection__: it is a second operator to specifythe keys of the document we want to be returned see [db.collection.find()](https://docs.mongodb.com/manual/reference/method/db.collection.find/#find-projection)\n",
    "\n",
    "OUTPUT:\n",
    "- __cursor__: by default we obtain a cursor which is a list (iterable object) of the documents matching our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_from_mongo(mongo_db, mongo_db_coll, return_cursor=False,\n",
    "                    criteria=None, projection=None, **mongo_conn_kw):\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)    \n",
    "    db = pymongo.database.Database(client, mongo_db)\n",
    "    coll = db.get_collection(mongo_db_coll)\n",
    "\n",
    "    if criteria is None:\n",
    "        criteria = {}\n",
    "    \n",
    "    if projection is None:\n",
    "        cursor = coll.find(criteria)\n",
    "    else:\n",
    "        cursor = coll.find(criteria, projection)\n",
    "    \n",
    "    if return_cursor:\n",
    "        return cursor\n",
    "    else:\n",
    "        return [ item for item in cursor ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_to_mongo()\n",
    "INPUT:\n",
    "- __mongo_db__: database in which we have the cllection we are interested in (string)\n",
    "- __mongo_db_coll__: collection in which we have the documents we are interested in (string)\n",
    "- __return_cursor__: if set to TRUE return a cursor object which is a list of the documents that match our query\n",
    "- __criteria__: it is the query icluded in {} parenthesis an written in javascript syntax (same as Monngo Shell)\n",
    "- __projection__: it is a second operator to specifythe keys of the document we want to be returned see [db.collection.find()](https://docs.mongodb.com/manual/reference/method/db.collection.find/#find-projection)\n",
    "\n",
    "OUTPUT:\n",
    "- __cursor__: by default we obtain a cursor which is a list (iterable object) of the documents matching our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_mongo(data, mongo_db, mongo_db_coll, **mongo_conn_kw):\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    \n",
    "    db = pymongo.database.Database(client, mongo_db)\n",
    "     \n",
    "    coll = db.get_collection(mongo_db_coll)\n",
    "\n",
    "    return coll.insert(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### natural language processing functions\n",
    "- _processTweet()_: process the tweet text cleaning it; take and return the tweet __text__\n",
    "- _processWord()_: process a single word; take and return __single word__\n",
    "- _getStopWordList()_: take a __text file__ containing english stopwords and return a __list of stopwords__\n",
    "- _getWordsVector()_: teke the __text__ of a tweet preprocess it and return a __list of words__ contained in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet['text'].lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert retweet\n",
    "    tweet = re.sub('(rt\\s)@[^\\s]+','RETWEET',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def processWord(w):\n",
    "    #look for 2 or more repetitions of character in a word and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    w = pattern.sub(r\"\\1\\1\", w)\n",
    "    #strip punctuation\n",
    "    w = w.strip('\\'\"?,.')\n",
    "    #check if the word starts with an alphabet\n",
    "    val = re.search(r\"^[a-zA-Z][a-zA-Z0-9-]*$\", w)\n",
    "    if val is None:\n",
    "        w = 'ABC'\n",
    "    return w\n",
    "\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    st = open(stopWordListFileName, 'r')\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    stopWords.append('RETWEET')\n",
    "    stopWords.append('ABC')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "\n",
    "def getWordsVector(tweet):\n",
    "    # initialize vector\n",
    "    wordsVector = []\n",
    "    #initialize stopWords\n",
    "    stopWords = getStopWordList('data/stopwords.txt')\n",
    "    #process tweet and split into words\n",
    "    tweet = processTweet(tweet)\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = processWord(w)\n",
    "        if w in stopWords:\n",
    "            continue\n",
    "        else:\n",
    "            wordsVector.append(w.lower())\n",
    "    return wordsVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @StepInPuddles: \"See through the trees\" alcohol marker pen and watercolour 8\" X 11\" please retweet if you like https://t.co/tbtrExK9rW\n",
      "processTweet()\n",
      "RETWEET \"see through the trees\" alcohol marker pen and watercolour 8\" x 11\" please retweet if you like URL\n",
      "processWord()\n",
      "['RETWEET', 'see', 'through', 'the', 'trees', 'alcohol', 'marker', 'pen', 'and', 'watercolour', 'ABC', 'x', 'ABC', 'please', 'retweet', 'if', 'you', 'like', 'URL']\n",
      "getStopWordList()\n",
      "getWordsVector()\n",
      "['trees', 'alcohol', 'marker', 'pen', 'watercolour', 'please', 'retweet']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "tweet = {\"text\" : \"RT @StepInPuddles: \\\"See through the trees\\\" alcohol marker pen and watercolour 8\\\" X 11\\\" please retweet if you like https://t.co/tbtrExK9rW\"}\n",
    "print tweet['text']\n",
    "print \"processTweet()\"\n",
    "processed_text = processTweet(tweet)\n",
    "print processed_text\n",
    "print \"processWord()\"\n",
    "words_list = []\n",
    "for w in processed_text.split():\n",
    "    w_processed = processWord(w)\n",
    "    words_list.append(w_processed)\n",
    "print words_list\n",
    "print \"getStopWordList()\"\n",
    "stwl = getStopWordList('data/stopwords.txt')\n",
    "print \"getWordsVector()\"\n",
    "wv = getWordsVector(tweet)\n",
    "\n",
    "print wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_entities_from_collection()\n",
    "INPUT:\n",
    "- __cursor_object__: Python cursor obgect returned as result of a MongoDB query\n",
    "\n",
    "OUTPUT:\n",
    "- __result__: returns a dictionary containing the 3 __lists__ of extracted entities, result = {\"words\": words, \"screen_names\": screen_names, \"hashtags\": hashtags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_entities_from_collection(cursor_object):\n",
    "    words = []\n",
    "    screen_names = []\n",
    "    hashtags = []\n",
    "    \n",
    "    result = {\"words\": words, \"screen_names\": screen_names, \"hashtags\": hashtags}\n",
    "\n",
    "    for tweet in cursor_object:\n",
    "        \n",
    "        wordsVector = getWordsVector(tweet)\n",
    "        for word in wordsVector:\n",
    "            words.append(word)\n",
    "\n",
    "        for user_mention in tweet['entities']['user_mentions']: \n",
    "            screen_names.append(user_mention['screen_name'])\n",
    "\n",
    "        for hashtag in tweet['entities']['hashtags']:\n",
    "            hashtags.append(hashtag['text'].lower())\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the functions\n",
    "\n",
    "Today it's July 28 2016; so far we have streamed data with the keyword \"tobacco\" for 5 days (\"keywords_list\" in MongoDB).\n",
    "We are going to explore this collection so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count occurencies of collection entities: words, hashtags, screen_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "| Word            | Count |\n",
      "+-----------------+-------+\n",
      "| health          | 30163 |\n",
      "| cancer          | 17523 |\n",
      "| smoke           | 11441 |\n",
      "| weed            |  8456 |\n",
      "| smoking         |  7700 |\n",
      "| alcohol         |  5880 |\n",
      "| da              |  5805 |\n",
      "| marijuana       |  5534 |\n",
      "| time            |  3745 |\n",
      "| women           |  3573 |\n",
      "| breast          |  3572 |\n",
      "| care            |  3268 |\n",
      "| mental          |  2906 |\n",
      "| cannabis        |  2854 |\n",
      "| stop            |  2834 |\n",
      "| survivors       |  2828 |\n",
      "| looking         |  2486 |\n",
      "| people          |  2422 |\n",
      "| path            |  2226 |\n",
      "| blocking        |  2150 |\n",
      "| quic            |  2142 |\n",
      "| medical         |  2139 |\n",
      "| via             |  2121 |\n",
      "| addiction       |  2080 |\n",
      "| minutes         |  2070 |\n",
      "| toby            |  1979 |\n",
      "| spent           |  1975 |\n",
      "| sandwhich       |  1935 |\n",
      "| mf              |  1920 |\n",
      "| bread           |  1916 |\n",
      "| ham             |  1915 |\n",
      "| drinking        |  1864 |\n",
      "| look            |  1703 |\n",
      "| zika            |  1692 |\n",
      "| love            |  1685 |\n",
      "| free            |  1682 |\n",
      "| drink           |  1676 |\n",
      "| life            |  1626 |\n",
      "| lung            |  1577 |\n",
      "| app             |  1516 |\n",
      "| day             |  1460 |\n",
      "| job             |  1431 |\n",
      "| issue           |  1414 |\n",
      "| help            |  1372 |\n",
      "| reps            |  1363 |\n",
      "| pipe            |  1299 |\n",
      "| fitness         |  1263 |\n",
      "| statement       |  1238 |\n",
      "| prayforhayes    |  1209 |\n",
      "| healthtipsbymsg |  1177 |\n",
      "| brother         |  1176 |\n",
      "| meet            |  1175 |\n",
      "| check           |  1154 |\n",
      "| news            |  1151 |\n",
      "| ur              |  1140 |\n",
      "| home            |  1113 |\n",
      "| ribs            |  1112 |\n",
      "| fractured       |  1098 |\n",
      "| bruised         |  1098 |\n",
      "| concussion      |  1096 |\n",
      "| crash           |  1096 |\n",
      "| montel          |  1094 |\n",
      "| wanna           |  1071 |\n",
      "| williams        |  1070 |\n",
      "| rt              |  1069 |\n",
      "| plant           |  1065 |\n",
      "| officials       |  1040 |\n",
      "| inflatable      |   993 |\n",
      "| cmon            |   988 |\n",
      "| cigarette       |   973 |\n",
      "| public          |   971 |\n",
      "| detained        |   967 |\n",
      "| thank           |   945 |\n",
      "| hiring          |   934 |\n",
      "| healthy         |   927 |\n",
      "| virgo           |   922 |\n",
      "| tips            |   907 |\n",
      "| food            |   888 |\n",
      "| raise           |   888 |\n",
      "| jobs            |   869 |\n",
      "| germany         |   850 |\n",
      "| join            |   846 |\n",
      "| benefits        |   837 |\n",
      "| lol             |   823 |\n",
      "| please          |   821 |\n",
      "| mytraining      |   819 |\n",
      "| bad             |   803 |\n",
      "| bitch           |   798 |\n",
      "| drunk           |   798 |\n",
      "| called          |   798 |\n",
      "| diet            |   789 |\n",
      "| florida         |   779 |\n",
      "| shit            |   770 |\n",
      "| fuck            |   769 |\n",
      "| afford          |   764 |\n",
      "| risk            |   763 |\n",
      "| taking          |   748 |\n",
      "| break           |   737 |\n",
      "| tryna           |   732 |\n",
      "| thanks          |   729 |\n",
      "| sagittarius     |   724 |\n",
      "| friends         |   722 |\n",
      "| live            |   718 |\n",
      "| support         |   716 |\n",
      "| months          |   716 |\n",
      "| universal       |   712 |\n",
      "| momma           |   711 |\n",
      "| smells          |   702 |\n",
      "| study           |   702 |\n",
      "| buy             |   701 |\n",
      "| research        |   700 |\n",
      "| black           |   699 |\n",
      "| pray            |   697 |\n",
      "| apple           |   697 |\n",
      "| tough           |   695 |\n",
      "| accidentally    |   694 |\n",
      "| healthcare      |   687 |\n",
      "| tyler           |   684 |\n",
      "| pour            |   682 |\n",
      "| hey             |   678 |\n",
      "| mixed           |   677 |\n",
      "| fan             |   677 |\n",
      "| pot             |   664 |\n",
      "| bigger          |   661 |\n",
      "| gun             |   649 |\n",
      "| video           |   648 |\n",
      "| test            |   645 |\n",
      "| times           |   628 |\n",
      "| filled          |   626 |\n",
      "| doors           |   626 |\n",
      "| god             |   617 |\n",
      "| blessings       |   616 |\n",
      "| read            |   613 |\n",
      "| physical        |   608 |\n",
      "| kids            |   607 |\n",
      "| opportunities   |   602 |\n",
      "| eat             |   601 |\n",
      "| night           |   598 |\n",
      "| lot             |   598 |\n",
      "| world           |   591 |\n",
      "| cause           |   589 |\n",
      "| girl            |   589 |\n",
      "| store           |   588 |\n",
      "| taurus          |   583 |\n",
      "| money           |   580 |\n",
      "| weight          |   576 |\n",
      "| def             |   576 |\n",
      "| nursing         |   574 |\n",
      "| treatment       |   572 |\n",
      "| blunt           |   570 |\n",
      "+-----------------+-------+\n",
      "+-----------------+-------+\n",
      "| Screen Name     | Count |\n",
      "+-----------------+-------+\n",
      "| BritneyO__      |  2319 |\n",
      "| HayesGrier      |  2296 |\n",
      "| HoesEnvy__JeAir |  1891 |\n",
      "| Gurmeetramrahim |  1293 |\n",
      "| twistmag        |  1194 |\n",
      "| hoecry          |  1124 |\n",
      "| wsoctv          |  1079 |\n",
      "| dog_rates       |   984 |\n",
      "| zodiaddicted    |   639 |\n",
      "| GoIfMedia       |   629 |\n",
      "| _CollegeHumor_  |   589 |\n",
      "| Ciao_belaa      |   565 |\n",
      "| chrissyteigen   |   558 |\n",
      "| FUCCl           |   434 |\n",
      "| wizkhalifa      |   390 |\n",
      "| DrJillStein     |   351 |\n",
      "| TreMelvin       |   339 |\n",
      "| _kweenm         |   333 |\n",
      "| Louis_Tomlinson |   289 |\n",
      "| Taniel          |   287 |\n",
      "| GreenRushEBay   |   282 |\n",
      "| HillaryClinton  |   273 |\n",
      "| YouTube         |   271 |\n",
      "| EBONYMag        |   269 |\n",
      "| OTCReporter     |   267 |\n",
      "| blackphabio     |   261 |\n",
      "| buddyhield      |   242 |\n",
      "| AP              |   233 |\n",
      "| thekingbrand420 |   232 |\n",
      "| realDonaldTrump |   222 |\n",
      "| RobHillSr       |   217 |\n",
      "| BestofCancer    |   211 |\n",
      "| RealAliciaGoku  |   205 |\n",
      "| nooR_inSan      |   189 |\n",
      "| GreenRushMVPs   |   187 |\n",
      "| The_Nenye       |   183 |\n",
      "| Cancer_A7       |   177 |\n",
      "| PizzaPartyBen   |   172 |\n",
      "| CNN             |   167 |\n",
      "| THCVibes420     |   166 |\n",
      "| okstephenadd    |   154 |\n",
      "| 420Humor        |   150 |\n",
      "| FlowKana        |   147 |\n",
      "| 911well         |   147 |\n",
      "| GreenRushSF     |   147 |\n",
      "| gpdb            |   146 |\n",
      "| TylerArmes      |   141 |\n",
      "| wearebne        |   141 |\n",
      "| wesleystromberg |   137 |\n",
      "| THEMMEXCHANGE   |   132 |\n",
      "| TheHPFacts      |   132 |\n",
      "| ganjababy4real  |   124 |\n",
      "| Kim_Fernandez81 |   123 |\n",
      "| WeedSpotlight   |   120 |\n",
      "| NECROMANClNG    |   119 |\n",
      "| HouseGOP        |   118 |\n",
      "| RepStevenSmith  |   117 |\n",
      "| schoolfession   |   115 |\n",
      "| H8onjacen       |   115 |\n",
      "| crystxlb        |   109 |\n",
      "| pressure        |   106 |\n",
      "| IlluminatedDope |   102 |\n",
      "| ShellzTopNotch  |   101 |\n",
      "| HIGH_TIMES_Mag  |   100 |\n",
      "| Jamster_33      |   100 |\n",
      "| casieapril      |    99 |\n",
      "| Fact            |    99 |\n",
      "| 520Promotion    |    98 |\n",
      "| LifeAsADabber   |    97 |\n",
      "| babitatyagi0    |    94 |\n",
      "| willam          |    94 |\n",
      "| GeniusPothead   |    93 |\n",
      "| MinaaBe         |    93 |\n",
      "| SenatorReid     |    91 |\n",
      "| MarshaCollier   |    90 |\n",
      "| XSTROLOGY       |    88 |\n",
      "| NoChillPosts    |    87 |\n",
      "| youngblackcon   |    86 |\n",
      "| thenickcolletti |    86 |\n",
      "| TheRoot         |    85 |\n",
      "| CynthiaEriVo    |    85 |\n",
      "| NiallOfficial   |    84 |\n",
      "| MSGAllTrading   |    83 |\n",
      "| Etsy            |    83 |\n",
      "| healthwomeninfo |    82 |\n",
      "| LifeWithAlcohol |    81 |\n",
      "| dril            |    78 |\n",
      "| SKC_Europe      |    76 |\n",
      "| princessneha23  |    76 |\n",
      "| Cancer_F6       |    76 |\n",
      "| ScottGreenfield |    75 |\n",
      "| WeedFeed        |    74 |\n",
      "| LeahRBoss       |    73 |\n",
      "| anxielous       |    72 |\n",
      "| BernieSanders   |    72 |\n",
      "| JennyShellby    |    70 |\n",
      "| poojakamboj176  |    70 |\n",
      "| kaitlynserna    |    69 |\n",
      "| CuntryCorner    |    67 |\n",
      "| schumelli1      |    67 |\n",
      "| XXL             |    66 |\n",
      "| MIT             |    66 |\n",
      "| BarackObama     |    66 |\n",
      "| NancyPelosi     |    65 |\n",
      "| Cancer_Quotes   |    64 |\n",
      "| SenBlumenthal   |    64 |\n",
      "| CraftySchmantzy |    64 |\n",
      "| KevinHart4real  |    64 |\n",
      "| GMA             |    63 |\n",
      "| POTUS           |    63 |\n",
      "| RiffRaff1971    |    63 |\n",
      "| MikePLong       |    62 |\n",
      "| CassieEl4       |    62 |\n",
      "| IreneMichaels   |    61 |\n",
      "| OrgyNextDoor    |    61 |\n",
      "| healthybeauty8  |    61 |\n",
      "| Alyssa_Milano   |    60 |\n",
      "| UrbanEngIish    |    60 |\n",
      "| WeedGifss       |    60 |\n",
      "| ColIegeStudent  |    60 |\n",
      "| KEEMSTAR        |    60 |\n",
      "| ABC             |    59 |\n",
      "| janinebucks     |    58 |\n",
      "| DrakeBible_     |    58 |\n",
      "| fucktyler       |    58 |\n",
      "| SPoonia777      |    57 |\n",
      "| TrueChatInc     |    56 |\n",
      "| Oxiunity        |    56 |\n",
      "| AthForGod       |    56 |\n",
      "| UNEP_EU         |    55 |\n",
      "| mellberr        |    54 |\n",
      "| Feelgood_420    |    54 |\n",
      "| GreenRushPen    |    54 |\n",
      "| kamalmeet7      |    53 |\n",
      "| kwonsoonshine   |    53 |\n",
      "| theNCI          |    52 |\n",
      "| NatGeo          |    52 |\n",
      "| aleeyu_liver    |    52 |\n",
      "| OhMyHazzaa      |    50 |\n",
      "| aalidah         |    50 |\n",
      "| Pritpal77777    |    49 |\n",
      "| Way_Things_Work |    48 |\n",
      "| HealthRanger    |    48 |\n",
      "| TheLadBible     |    47 |\n",
      "| MAPS            |    46 |\n",
      "| MahirZeynalov   |    45 |\n",
      "| FilmFatale_NYC  |    45 |\n",
      "| MyPromosTooLit  |    44 |\n",
      "| AHealthBlog     |    44 |\n",
      "| LiquidHbox      |    44 |\n",
      "+-----------------+-------+\n",
      "+--------------------------------+-------+\n",
      "| Hashtag                        | Count |\n",
      "+--------------------------------+-------+\n",
      "| health                         |  5770 |\n",
      "| cannabis                       |  1884 |\n",
      "| cancer                         |  1637 |\n",
      "| healthtipsbymsg                |  1484 |\n",
      "| marijuana                      |  1371 |\n",
      "| prayforhayes                   |  1213 |\n",
      "| weed                           |  1205 |\n",
      "| job                            |  1185 |\n",
      "| hiring                         |  1062 |\n",
      "| fitness                        |   816 |\n",
      "| jobs                           |   753 |\n",
      "| ebonychat                      |   631 |\n",
      "| healthcare                     |   553 |\n",
      "| nursing                        |   507 |\n",
      "| diet                           |   477 |\n",
      "| careerarc                      |   435 |\n",
      "| seeds                          |   408 |\n",
      "| mmj                            |   364 |\n",
      "| deals                          |   357 |\n",
      "| healthy                        |   330 |\n",
      "| blackwomendidthat              |   327 |\n",
      "| california                     |   324 |\n",
      "| news                           |   319 |\n",
      "| zika                           |   316 |\n",
      "| nutrition                      |   314 |\n",
      "| priorities                     |   303 |\n",
      "| weightloss                     |   277 |\n",
      "| wellness                       |   245 |\n",
      "| workout                        |   225 |\n",
      "| addiction                      |   212 |\n",
      "| alcohol                        |   204 |\n",
      "| vaping                         |   201 |\n",
      "| acne                           |   196 |\n",
      "| beauty                         |   183 |\n",
      "| demsinphilly                   |   182 |\n",
      "| medical                        |   178 |\n",
      "| mentalhealth                   |   177 |\n",
      "| effects                        |   175 |\n",
      "| of                             |   169 |\n",
      "| rt                             |   168 |\n",
      "| yoga                           |   168 |\n",
      "| to                             |   161 |\n",
      "| smoking                        |   149 |\n",
      "| love                           |   147 |\n",
      "| imwithhernow                   |   146 |\n",
      "| food                           |   138 |\n",
      "| mme                            |   138 |\n",
      "| firemeup                       |   138 |\n",
      "| vegan                          |   135 |\n",
      "| exercise                       |   132 |\n",
      "| buy                            |   132 |\n",
      "| soundcloud                     |   127 |\n",
      "| gpdb                           |   125 |\n",
      "| youth                          |   125 |\n",
      "| summersoup                     |   125 |\n",
      "| pot                            |   121 |\n",
      "| fatloss                        |   119 |\n",
      "| online                         |   118 |\n",
      "| medicine                       |   117 |\n",
      "| periscope                      |   110 |\n",
      "| fit                            |   106 |\n",
      "| science                        |   106 |\n",
      "| motivation                     |   104 |\n",
      "| mobile                         |   102 |\n",
      "| blackmentalhealth              |   101 |\n",
      "| 520promo                       |    99 |\n",
      "| hillarystongue                 |    99 |\n",
      "| apps                           |    99 |\n",
      "| vape                           |    98 |\n",
      "| ptsd                           |    94 |\n",
      "| bigolive                       |    94 |\n",
      "| investment                     |    90 |\n",
      "| diabetes                       |    90 |\n",
      "| smoke                          |    88 |\n",
      "| solution                       |    88 |\n",
      "| icnm16                         |    87 |\n",
      "| improveyourselfinfivewords     |    86 |\n",
      "| prevention                     |    83 |\n",
      "| imwithher                      |    82 |\n",
      "| voterid                        |    82 |\n",
      "| rickynorwood                   |    79 |\n",
      "| fashion                        |    77 |\n",
      "| gardening                      |    76 |\n",
      "| nowplaying                     |    75 |\n",
      "| momo420                        |    75 |\n",
      "| antiaging                      |    75 |\n",
      "| c0nvey                         |    74 |\n",
      "| doyourjob                      |    74 |\n",
      "| lifestyle                      |    71 |\n",
      "| getfit                         |    70 |\n",
      "| ketosis                        |    70 |\n",
      "| gym                            |    70 |\n",
      "| etsymntt                       |    69 |\n",
      "| usa                            |    69 |\n",
      "| oregon                         |    68 |\n",
      "| dncinphl                       |    68 |\n",
      "| can                            |    68 |\n",
      "| fridayfeeling                  |    68 |\n",
      "| hope                           |    67 |\n",
      "| ebislove                       |    67 |\n",
      "| peace                          |    67 |\n",
      "| recovery                       |    66 |\n",
      "| tech                           |    65 |\n",
      "| ascendant                      |    65 |\n",
      "| mediumcoeli                    |    65 |\n",
      "| grow                           |    64 |\n",
      "| veterans                       |    63 |\n",
      "| cosmetics                      |    63 |\n",
      "| google                         |    63 |\n",
      "| blog                           |    63 |\n",
      "| art                            |    62 |\n",
      "| life                           |    61 |\n",
      "| beautychat                     |    61 |\n",
      "| florida                        |    61 |\n",
      "| strains                        |    61 |\n",
      "| women                          |    61 |\n",
      "| fitfam                         |    60 |\n",
      "| dream                          |    60 |\n",
      "| dogs                           |    59 |\n",
      "| healthyliving                  |    59 |\n",
      "| travel                         |    58 |\n",
      "| photo                          |    58 |\n",
      "| mustweed                       |    58 |\n",
      "| brain                          |    57 |\n",
      "| growing                        |    57 |\n",
      "| for                            |    56 |\n",
      "| sativa                         |    56 |\n",
      "| arizona                        |    56 |\n",
      "| hearourvoiceschangeyourchoices |    55 |\n",
      "| digitalhealth                  |    55 |\n",
      "| oil                            |    55 |\n",
      "| seed                           |    54 |\n",
      "| skin                           |    53 |\n",
      "| thisnewdietis                  |    53 |\n",
      "| illinois                       |    52 |\n",
      "| where                          |    50 |\n",
      "| breastcancer                   |    50 |\n",
      "| fitbit                         |    49 |\n",
      "| ecigs                          |    49 |\n",
      "| pharma                         |    49 |\n",
      "| healthampfitness               |    49 |\n",
      "| msg4yourhealth                 |    49 |\n",
      "| zikavirus                      |    49 |\n",
      "| skincare                       |    49 |\n",
      "| how                            |    48 |\n",
      "| best                           |    48 |\n",
      "| az                             |    48 |\n",
      "| cigar                          |    47 |\n",
      "| flintwatercrisis               |    47 |\n",
      "| natural                        |    46 |\n",
      "+--------------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "cur = load_from_mongo('streamingAPI', 'top_occurrences0', return_cursor=True)\n",
    "\n",
    "results = extract_entities_from_collection(cur)\n",
    "\n",
    "for label, data in (('Word', results[\"words\"]), \n",
    "                    ('Screen Name', results[\"screen_names\"]), \n",
    "                    ('Hashtag', results[\"hashtags\"])):\n",
    "    pt = PrettyTable(field_names=[label, 'Count']) \n",
    "    c = Counter(data)\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:150] ]\n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print pt\n",
    "    \n",
    "# if 'KeyError: 'text'' is raised execute the following query in mongo\n",
    "# db.getCollection('top_occurrences0').remove({\"text\": { $exists: false }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hashtags is a sub-class of words (because we consider the hashtag also as a word), we want to update the stream list with the top words.\n",
    "Problem: when adjective occurr would be more helpfull to have __n-grams__.\n",
    "\n",
    "__TODO__: check for n-grams (focus on 3-grams and 5-grams)\n",
    "\n",
    "[n-grams](https://marcobonzanini.com/2015/03/17/mining-twitter-data-with-python-part-3-term-frequencies/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
